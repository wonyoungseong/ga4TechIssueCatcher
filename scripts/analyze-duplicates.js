#!/usr/bin/env node

/**
 * Analyze duplicate URLs in CSV
 */

import fs from 'fs/promises';
import { parse } from 'csv-parse/sync';

async function analyzeDuplicates() {
  const csvPath = '/Users/seong-won-yeong/Dev/ga4TechIssueCatcher/data/properties-import.csv';

  // Read CSV file
  const fileContent = await fs.readFile(csvPath, 'utf-8');

  // Parse CSV
  const records = parse(fileContent, {
    columns: true,
    skip_empty_lines: true,
    trim: true,
    relax_column_count: true
  });

  console.log(`\nğŸ“Š Total rows in CSV: ${records.length}\n`);

  // Track URLs
  const urlCounts = new Map();
  const urlRecords = new Map();

  for (const record of records) {
    const url = record['ëŒ€í‘œ URLs']?.trim();

    if (!url) continue;

    if (!urlCounts.has(url)) {
      urlCounts.set(url, 0);
      urlRecords.set(url, []);
    }

    urlCounts.set(url, urlCounts.get(url) + 1);
    urlRecords.get(url).push({
      propertyName: record['ì†ì„±ëª…'],
      brand: record['ê³„ì •ëª…'],
      ga4Id: record['Web Stream Measurement ID'],
      gtmId: record['Web GTM Pubilic ID']
    });
  }

  // Find duplicates
  const duplicates = [];
  for (const [url, count] of urlCounts) {
    if (count > 1) {
      duplicates.push({ url, count, records: urlRecords.get(url) });
    }
  }

  console.log(`ğŸ” Found ${duplicates.length} duplicate URLs:\n`);
  console.log('='.repeat(80));

  for (const { url, count, records } of duplicates) {
    console.log(`\nğŸ“ URL: ${url}`);
    console.log(`   ì¤‘ë³µ íšŸìˆ˜: ${count}íšŒ`);
    console.log(`   ì¤‘ë³µ í•­ëª©ë“¤:`);

    records.forEach((rec, idx) => {
      console.log(`   ${idx + 1}. ${rec.propertyName}`);
      console.log(`      - Brand: ${rec.brand}`);
      console.log(`      - GA4 ID: ${rec.ga4Id}`);
      console.log(`      - GTM ID: ${rec.gtmId}`);
    });
  }

  console.log('\n' + '='.repeat(80));
  console.log(`\nâœ… ì´ ${duplicates.length}ê°œì˜ ì¤‘ë³µ URL`);
  console.log(`   CSV ì „ì²´: ${records.length}ê°œ â†’ Unique: ${urlCounts.size}ê°œ`);
  console.log(`   ì¤‘ë³µìœ¼ë¡œ ì œê±°ëœ í•­ëª©: ${records.length - urlCounts.size}ê°œ\n`);
}

analyzeDuplicates();
